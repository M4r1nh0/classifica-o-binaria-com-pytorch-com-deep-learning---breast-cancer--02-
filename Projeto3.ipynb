{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Projeto3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNhZlmBEs8da83i0dhN1TEm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M4r1nh0/classifica-o-binaria-com-pytorch-com-deep-learning---breast-cancer--02-/blob/master/Projeto3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCP1agjT_ru5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "0f977d4e-2a26-4b65-9e6a-835ae1bb36e1"
      },
      "source": [
        "!pip install 'skorch==0.7.0'\n",
        "!pip install 'torch==1.4.0'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting skorch==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/1e/cc4e1f23cd1faab06672f309e0857294aaa80c5f84670f4d3d19b08ab10b/skorch-0.7.0-py3-none-any.whl (105kB)\n",
            "\r\u001b[K     |███                             | 10kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 20kB 1.9MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 40kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 51kB 2.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 61kB 2.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 71kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 81kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 92kB 3.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 102kB 3.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from skorch==0.7.0) (1.4.1)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.6/dist-packages (from skorch==0.7.0) (4.41.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from skorch==0.7.0) (0.22.2.post1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from skorch==0.7.0) (0.8.7)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from skorch==0.7.0) (1.18.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.1->skorch==0.7.0) (0.16.0)\n",
            "Installing collected packages: skorch\n",
            "Successfully installed skorch-0.7.0\n",
            "Collecting torch==1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4MB 19kB/s \n",
            "\u001b[31mERROR: torchvision 0.7.0+cu101 has requirement torch==1.6.0, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "Successfully installed torch-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rINUuAJ2_0n4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd #Panda pro dataset\n",
        "import numpy as np #numpy pros vetores\n",
        "import torch.nn as nn #torch por neural network\n",
        "from skorch import NeuralNetBinaryClassifier \n",
        "import torch\n",
        "import skorch\n",
        "import torch.nn.functional as F #trabalhamos com função de ativação\n",
        "from sklearn.model_selection import GridSearchCV #cross validation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUWIgOGLAXvo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "57d07934-acc7-4f37-fad6-66a4e95ececb"
      },
      "source": [
        "torch.__version__\n",
        "skorch.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0.7.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AocchBjlAiM8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "06b0a9d0-3983-4b98-b17a-ab2478df1165"
      },
      "source": [
        "np.random.seed(123)\n",
        "torch.manual_seed(123)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fc2c1bd6350>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpcCfEGVFFd9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "previsores = pd.read_csv('entradas_breast.csv')\n",
        "classes = pd.read_csv('saidas_breast.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F2Iuf9FFwTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "previsores = np.array(previsores, dtype= 'float32')\n",
        "classes = np.array(classes, dtype= 'float32').squeeze(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rackIukuGFp6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb2c43c9-a6c4-478c-a22d-edaa20af30f9"
      },
      "source": [
        "previsores.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569, 30)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NgAd91LGHT8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f5b4f5a7-eb1e-491c-8ea7-3504ba7c2714"
      },
      "source": [
        "classes.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJmEMCZLGIfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class classificador_torch(nn.Module):\n",
        "  def __init__(self, activation, neurons, initializer):\n",
        "    super().__init__()\n",
        "    self.dense0 = nn.Linear(30, neurons)\n",
        "    initializer(self.dense0.weight)\n",
        "    self.activation0 = activation\n",
        "    self.dense1 = nn.Linear(neurons, neurons)\n",
        "    initializer(self.dense1.weight)\n",
        "    self.activation1 = activation\n",
        "    self.dense2 = nn.Linear(neurons, 1)\n",
        "    initializer(self.dense2.weight)\n",
        "    self.output = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, X):\n",
        "    X = self.dense0(X)\n",
        "    X = self.activation0(X)\n",
        "    X = self.dense1(X)\n",
        "    X = self.activation1(X)\n",
        "    X = self.dense2(X)\n",
        "    X = self.output(X)\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9shXRjzKG8UV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classificado_sklearn = NeuralNetBinaryClassifier(module=classificador_torch,\n",
        "                                                 lr= 0.001,\n",
        "                                                 optimizer__weight_decay =0.0001,\n",
        "                                                 train_split=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwGsVijVIRQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parms = {'batch_size': [10, 30],\n",
        "         'max_epochs': [50, 100],\n",
        "         'optimizer': [torch.optim.Adam],#torch.optim.SGD\n",
        "         'criterion': [torch.nn.BCELoss], #torch.nn.HingeEmbeddingLoss],\n",
        "         'module__activation': [F.relu],# F.tanh],\n",
        "         'module__neurons': [16],#8\n",
        "         'module__initializer': [torch.nn.init.uniform_], #torch.nn.init.normal_]\n",
        "         }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbAgli-QKqbP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "076f7aaa-b27e-4a39-b8b1-327ba0a3ae4c"
      },
      "source": [
        "grid_search = GridSearchCV(estimator=classificado_sklearn, param_grid=parms, scoring= 'accuracy', cv = 2)\n",
        "grid_search = grid_search.fit(previsores,classes)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m10.3130\u001b[0m  0.1330\n",
            "      2       10.3130  0.0391\n",
            "      3       10.3130  0.0381\n",
            "      4       10.3130  0.0395\n",
            "      5       10.3130  0.0450\n",
            "      6       10.3130  0.0382\n",
            "      7       10.3130  0.0378\n",
            "      8       10.3130  0.0416\n",
            "      9       10.3130  0.0378\n",
            "     10       10.3130  0.0392\n",
            "     11       10.3130  0.0394\n",
            "     12       10.3130  0.0385\n",
            "     13       10.3130  0.0386\n",
            "     14       10.3130  0.0387\n",
            "     15       10.3130  0.0383\n",
            "     16       10.3130  0.0438\n",
            "     17       10.3130  0.0415\n",
            "     18       10.3130  0.0378\n",
            "     19       10.3130  0.0381\n",
            "     20       10.3130  0.0388\n",
            "     21       10.3130  0.0385\n",
            "     22       10.3130  0.0399\n",
            "     23       10.3130  0.0388\n",
            "     24       10.3130  0.0391\n",
            "     25       10.3130  0.0408\n",
            "     26       10.3130  0.0439\n",
            "     27       10.3130  0.0376\n",
            "     28       10.3130  0.0384\n",
            "     29       10.3130  0.0387\n",
            "     30       10.3130  0.0384\n",
            "     31       10.3130  0.0384\n",
            "     32       10.3130  0.0378\n",
            "     33       10.3130  0.0384\n",
            "     34       10.3130  0.0394\n",
            "     35       10.3130  0.0391\n",
            "     36       10.3130  0.0380\n",
            "     37       10.3130  0.0385\n",
            "     38       10.3130  0.0381\n",
            "     39       10.3130  0.0387\n",
            "     40       10.3130  0.0387\n",
            "     41       10.3130  0.0432\n",
            "     42       10.3130  0.0422\n",
            "     43       10.3130  0.0392\n",
            "     44       10.3130  0.0393\n",
            "     45       10.3130  0.0385\n",
            "     46       10.3130  0.0395\n",
            "     47       10.3130  0.0402\n",
            "     48       10.3130  0.0412\n",
            "     49       10.3130  0.0383\n",
            "     50       10.3130  0.0458\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m10.2768\u001b[0m  0.0354\n",
            "      2       10.2768  0.0380\n",
            "      3       10.2768  0.0394\n",
            "      4       10.2768  0.0382\n",
            "      5       10.2768  0.0407\n",
            "      6       10.2768  0.0382\n",
            "      7       10.2768  0.0386\n",
            "      8       10.2768  0.0384\n",
            "      9       10.2768  0.0385\n",
            "     10       10.2768  0.0381\n",
            "     11       10.2768  0.0525\n",
            "     12       10.2768  0.0402\n",
            "     13       10.2768  0.0383\n",
            "     14       10.2768  0.0444\n",
            "     15       10.2768  0.0428\n",
            "     16       10.2768  0.0411\n",
            "     17       10.2768  0.0383\n",
            "     18       10.2768  0.0388\n",
            "     19       10.2768  0.0382\n",
            "     20       10.2768  0.0406\n",
            "     21       10.2768  0.0468\n",
            "     22       10.2768  0.0385\n",
            "     23       10.2768  0.0386\n",
            "     24       10.2768  0.0388\n",
            "     25       10.2768  0.0386\n",
            "     26       10.2768  0.0384\n",
            "     27       10.2768  0.0385\n",
            "     28       10.2768  0.0389\n",
            "     29       10.2768  0.0384\n",
            "     30       10.2768  0.0385\n",
            "     31       10.2768  0.0400\n",
            "     32       10.2768  0.0419\n",
            "     33       10.2768  0.0385\n",
            "     34       10.2768  0.0384\n",
            "     35       10.2768  0.0383\n",
            "     36       10.2768  0.0387\n",
            "     37       10.2768  0.0391\n",
            "     38       10.2768  0.0400\n",
            "     39       10.2768  0.0402\n",
            "     40       10.2768  0.0387\n",
            "     41       10.2768  0.0382\n",
            "     42       10.2768  0.0388\n",
            "     43       10.2768  0.0388\n",
            "     44       10.2768  0.0450\n",
            "     45        \u001b[36m8.1635\u001b[0m  0.0409\n",
            "     46        \u001b[36m2.1135\u001b[0m  0.0388\n",
            "     47        \u001b[36m0.7116\u001b[0m  0.0386\n",
            "     48        \u001b[36m0.6149\u001b[0m  0.0389\n",
            "     49        \u001b[36m0.6135\u001b[0m  0.0389\n",
            "     50        \u001b[36m0.6119\u001b[0m  0.0391\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m10.3130\u001b[0m  0.0358\n",
            "      2       10.3130  0.0388\n",
            "      3       10.3130  0.0396\n",
            "      4       10.3130  0.0414\n",
            "      5       10.3130  0.0387\n",
            "      6       10.3130  0.0390\n",
            "      7       10.3130  0.0388\n",
            "      8       10.3130  0.0386\n",
            "      9       10.3130  0.0394\n",
            "     10       10.3130  0.0389\n",
            "     11       10.3130  0.0487\n",
            "     12       10.3130  0.0430\n",
            "     13       10.3130  0.0382\n",
            "     14       10.3130  0.0378\n",
            "     15       10.3130  0.0389\n",
            "     16       10.3130  0.0477\n",
            "     17       10.3130  0.0538\n",
            "     18       10.3130  0.0411\n",
            "     19       10.3130  0.0360\n",
            "     20       10.3130  0.0401\n",
            "     21       10.3130  0.0383\n",
            "     22       10.3130  0.0383\n",
            "     23       10.3130  0.0381\n",
            "     24       10.3130  0.0384\n",
            "     25       10.3130  0.0378\n",
            "     26       10.3130  0.0380\n",
            "     27       10.3130  0.0390\n",
            "     28       10.3130  0.0380\n",
            "     29       10.3130  0.0381\n",
            "     30       10.3130  0.0402\n",
            "     31       10.3130  0.0363\n",
            "     32       10.3130  0.0396\n",
            "     33       10.3130  0.0362\n",
            "     34       10.3130  0.0379\n",
            "     35       10.3130  0.0378\n",
            "     36       10.3130  0.0380\n",
            "     37       10.3130  0.0384\n",
            "     38       10.3130  0.0413\n",
            "     39       10.3130  0.0385\n",
            "     40       10.3130  0.0427\n",
            "     41       10.3130  0.0383\n",
            "     42       10.3130  0.0389\n",
            "     43       10.3130  0.0384\n",
            "     44       10.3130  0.0385\n",
            "     45       10.3130  0.0388\n",
            "     46       10.3130  0.0387\n",
            "     47       10.3130  0.0385\n",
            "     48       10.3130  0.0404\n",
            "     49       10.3130  0.0413\n",
            "     50       10.3130  0.0381\n",
            "     51       10.3130  0.0385\n",
            "     52       10.3130  0.0392\n",
            "     53       10.3130  0.0392\n",
            "     54       10.3130  0.0398\n",
            "     55       10.3130  0.0395\n",
            "     56       10.3130  0.0405\n",
            "     57       10.3130  0.0393\n",
            "     58       10.3130  0.0387\n",
            "     59       10.3130  0.0392\n",
            "     60       10.3130  0.0387\n",
            "     61       10.3130  0.0427\n",
            "     62       10.3130  0.0425\n",
            "     63       10.3130  0.0384\n",
            "     64       10.3130  0.0387\n",
            "     65       10.3130  0.0386\n",
            "     66       10.3130  0.0390\n",
            "     67       10.3130  0.0535\n",
            "     68       10.3130  0.0429\n",
            "     69       10.3130  0.0387\n",
            "     70       10.3130  0.0386\n",
            "     71       10.3130  0.0452\n",
            "     72       10.3130  0.0396\n",
            "     73       10.3130  0.0392\n",
            "     74       10.3130  0.0393\n",
            "     75       10.3130  0.0391\n",
            "     76       10.3130  0.0391\n",
            "     77       10.3130  0.0395\n",
            "     78       10.3130  0.0388\n",
            "     79       10.3130  0.0389\n",
            "     80       10.3130  0.0388\n",
            "     81       10.3130  0.0436\n",
            "     82       10.3130  0.0395\n",
            "     83       10.3130  0.0388\n",
            "     84       10.3130  0.0399\n",
            "     85       10.3130  0.0395\n",
            "     86       10.3130  0.0391\n",
            "     87       10.3130  0.0395\n",
            "     88       10.3130  0.0388\n",
            "     89       10.3130  0.0393\n",
            "     90       10.3130  0.0407\n",
            "     91       10.3130  0.0396\n",
            "     92       10.3130  0.0391\n",
            "     93       10.3130  0.0390\n",
            "     94       10.3130  0.0504\n",
            "     95       10.3130  0.0438\n",
            "     96       10.3130  0.0404\n",
            "     97       10.3130  0.0389\n",
            "     98       10.3130  0.0407\n",
            "     99       10.3130  0.0511\n",
            "    100       10.3130  0.0528\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m10.2768\u001b[0m  0.0366\n",
            "      2       10.2768  0.0389\n",
            "      3       10.2768  0.0383\n",
            "      4       10.2768  0.0385\n",
            "      5       10.2768  0.0390\n",
            "      6       10.2768  0.0370\n",
            "      7       10.2768  0.0464\n",
            "      8       10.2768  0.0453\n",
            "      9       10.2768  0.0374\n",
            "     10       10.2768  0.0384\n",
            "     11       10.2768  0.0381\n",
            "     12       10.2768  0.0391\n",
            "     13       10.2768  0.0389\n",
            "     14       10.2768  0.0403\n",
            "     15       10.2768  0.0426\n",
            "     16       10.2768  0.0386\n",
            "     17       10.2768  0.0386\n",
            "     18       10.2768  0.0383\n",
            "     19       10.2768  0.0387\n",
            "     20       10.2768  0.0378\n",
            "     21       10.2768  0.0401\n",
            "     22       10.2768  0.0393\n",
            "     23       10.2768  0.0445\n",
            "     24       10.2768  0.0383\n",
            "     25       10.2768  0.0382\n",
            "     26       10.2768  0.0386\n",
            "     27       10.2768  0.0398\n",
            "     28       10.2768  0.0401\n",
            "     29       10.2768  0.0422\n",
            "     30       10.2768  0.0383\n",
            "     31       10.2768  0.0395\n",
            "     32       10.2768  0.0375\n",
            "     33       10.2768  0.0390\n",
            "     34       10.2768  0.0386\n",
            "     35       10.2768  0.0382\n",
            "     36       10.2768  0.0390\n",
            "     37       10.2768  0.0539\n",
            "     38       10.2768  0.0473\n",
            "     39       10.2768  0.0411\n",
            "     40       10.2768  0.0387\n",
            "     41       10.2768  0.0387\n",
            "     42       10.2768  0.0385\n",
            "     43       10.2768  0.0430\n",
            "     44       10.2768  0.0390\n",
            "     45       10.2768  0.0440\n",
            "     46        \u001b[36m8.3004\u001b[0m  0.0383\n",
            "     47        \u001b[36m1.4168\u001b[0m  0.0392\n",
            "     48        \u001b[36m0.6119\u001b[0m  0.0388\n",
            "     49        \u001b[36m0.5909\u001b[0m  0.0420\n",
            "     50        \u001b[36m0.5791\u001b[0m  0.0395\n",
            "     51        \u001b[36m0.5643\u001b[0m  0.0387\n",
            "     52        \u001b[36m0.5541\u001b[0m  0.0378\n",
            "     53        \u001b[36m0.5370\u001b[0m  0.0386\n",
            "     54        \u001b[36m0.5242\u001b[0m  0.0381\n",
            "     55        \u001b[36m0.5188\u001b[0m  0.0400\n",
            "     56        \u001b[36m0.5068\u001b[0m  0.0442\n",
            "     57        \u001b[36m0.4993\u001b[0m  0.0466\n",
            "     58        \u001b[36m0.4960\u001b[0m  0.0386\n",
            "     59        \u001b[36m0.4906\u001b[0m  0.0387\n",
            "     60        \u001b[36m0.4865\u001b[0m  0.0385\n",
            "     61        \u001b[36m0.4826\u001b[0m  0.0388\n",
            "     62        \u001b[36m0.4792\u001b[0m  0.0392\n",
            "     63        \u001b[36m0.4753\u001b[0m  0.0390\n",
            "     64        \u001b[36m0.4704\u001b[0m  0.0396\n",
            "     65        \u001b[36m0.4671\u001b[0m  0.0406\n",
            "     66        \u001b[36m0.4619\u001b[0m  0.0450\n",
            "     67        \u001b[36m0.4577\u001b[0m  0.0392\n",
            "     68        \u001b[36m0.4396\u001b[0m  0.0389\n",
            "     69        0.4725  0.0394\n",
            "     70        \u001b[36m0.4324\u001b[0m  0.0388\n",
            "     71        \u001b[36m0.4277\u001b[0m  0.0446\n",
            "     72        \u001b[36m0.4147\u001b[0m  0.0391\n",
            "     73        0.4150  0.0394\n",
            "     74        \u001b[36m0.3957\u001b[0m  0.0387\n",
            "     75        \u001b[36m0.3842\u001b[0m  0.0391\n",
            "     76        0.3846  0.0396\n",
            "     77        \u001b[36m0.3705\u001b[0m  0.0433\n",
            "     78        \u001b[36m0.3671\u001b[0m  0.0388\n",
            "     79        \u001b[36m0.3568\u001b[0m  0.0397\n",
            "     80        0.3686  0.0501\n",
            "     81        \u001b[36m0.3514\u001b[0m  0.0400\n",
            "     82        \u001b[36m0.3386\u001b[0m  0.0399\n",
            "     83        0.3391  0.0392\n",
            "     84        \u001b[36m0.3279\u001b[0m  0.0394\n",
            "     85        0.3462  0.0385\n",
            "     86        0.3316  0.0366\n",
            "     87        \u001b[36m0.3158\u001b[0m  0.0387\n",
            "     88        \u001b[36m0.3090\u001b[0m  0.0464\n",
            "     89        0.3091  0.0414\n",
            "     90        \u001b[36m0.2982\u001b[0m  0.0395\n",
            "     91        0.3051  0.0389\n",
            "     92        \u001b[36m0.2921\u001b[0m  0.0391\n",
            "     93        \u001b[36m0.2872\u001b[0m  0.0402\n",
            "     94        0.2923  0.0469\n",
            "     95        \u001b[36m0.2748\u001b[0m  0.0411\n",
            "     96        \u001b[36m0.2738\u001b[0m  0.0396\n",
            "     97        \u001b[36m0.2655\u001b[0m  0.0397\n",
            "     98        \u001b[36m0.2652\u001b[0m  0.0387\n",
            "     99        \u001b[36m0.2637\u001b[0m  0.0389\n",
            "    100        \u001b[36m0.2558\u001b[0m  0.0394\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m10.3130\u001b[0m  0.0169\n",
            "      2       10.3130  0.0168\n",
            "      3       10.3130  0.0164\n",
            "      4       10.3130  0.0178\n",
            "      5       10.3130  0.0159\n",
            "      6       10.3130  0.0174\n",
            "      7       10.3130  0.0164\n",
            "      8       10.3130  0.0163\n",
            "      9       10.3130  0.0182\n",
            "     10       10.3130  0.0163\n",
            "     11       10.3130  0.0211\n",
            "     12       10.3130  0.0163\n",
            "     13       10.3130  0.0177\n",
            "     14       10.3130  0.0161\n",
            "     15       10.3130  0.0175\n",
            "     16       10.3130  0.0186\n",
            "     17       10.3130  0.0185\n",
            "     18       10.3130  0.0164\n",
            "     19       10.3130  0.0173\n",
            "     20       10.3130  0.0165\n",
            "     21       10.3130  0.0183\n",
            "     22       10.3130  0.0161\n",
            "     23       10.3130  0.0219\n",
            "     24       10.3130  0.0162\n",
            "     25       10.3130  0.0175\n",
            "     26       10.3130  0.0162\n",
            "     27       10.3130  0.0180\n",
            "     28       10.3130  0.0163\n",
            "     29       10.3130  0.0172\n",
            "     30       10.3130  0.0149\n",
            "     31       10.3130  0.0206\n",
            "     32       10.3130  0.0163\n",
            "     33       10.3130  0.0181\n",
            "     34       10.3130  0.0172\n",
            "     35       10.3130  0.0179\n",
            "     36       10.3130  0.0167\n",
            "     37       10.3130  0.0177\n",
            "     38       10.3130  0.0164\n",
            "     39       10.3130  0.0167\n",
            "     40       10.3130  0.0160\n",
            "     41       10.3130  0.0184\n",
            "     42       10.3130  0.0165\n",
            "     43       10.3130  0.0181\n",
            "     44       10.3130  0.0171\n",
            "     45       10.3130  0.0180\n",
            "     46       10.3130  0.0171\n",
            "     47       10.3130  0.0184\n",
            "     48       10.3130  0.0164\n",
            "     49       10.3130  0.0220\n",
            "     50       10.3130  0.0172\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m10.2768\u001b[0m  0.0138\n",
            "      2       10.2768  0.0163\n",
            "      3       10.2768  0.0188\n",
            "      4       10.2768  0.0166\n",
            "      5       10.2768  0.0172\n",
            "      6       10.2768  0.0172\n",
            "      7       10.2768  0.0163\n",
            "      8       10.2768  0.0174\n",
            "      9       10.2768  0.0162\n",
            "     10       10.2768  0.0187\n",
            "     11       10.2768  0.0159\n",
            "     12       10.2768  0.0181\n",
            "     13       10.2768  0.0164\n",
            "     14       10.2768  0.0163\n",
            "     15       10.2768  0.0168\n",
            "     16       10.2768  0.0217\n",
            "     17       10.2768  0.0168\n",
            "     18       10.2768  0.0163\n",
            "     19       10.2768  0.0165\n",
            "     20       10.2768  0.0171\n",
            "     21       10.2768  0.0184\n",
            "     22       10.2768  0.0198\n",
            "     23       10.2768  0.0181\n",
            "     24       10.2768  0.0188\n",
            "     25       10.2768  0.0165\n",
            "     26       10.2768  0.0159\n",
            "     27       10.2768  0.0163\n",
            "     28       10.2768  0.0162\n",
            "     29       10.2768  0.0170\n",
            "     30       10.2768  0.0163\n",
            "     31       10.2768  0.0179\n",
            "     32       10.2768  0.0167\n",
            "     33       10.2768  0.0171\n",
            "     34       10.2768  0.0160\n",
            "     35       10.2768  0.0179\n",
            "     36       10.2768  0.0176\n",
            "     37       10.2768  0.0172\n",
            "     38       10.2768  0.0175\n",
            "     39       10.2768  0.0173\n",
            "     40       10.2768  0.0165\n",
            "     41       10.2768  0.0174\n",
            "     42       10.2768  0.0167\n",
            "     43       10.2768  0.0170\n",
            "     44       10.2768  0.0166\n",
            "     45       10.2768  0.0183\n",
            "     46       10.2768  0.0170\n",
            "     47       10.2768  0.0175\n",
            "     48       10.2768  0.0169\n",
            "     49       10.2768  0.0179\n",
            "     50       10.2768  0.0164\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m10.3130\u001b[0m  0.0142\n",
            "      2       10.3130  0.0177\n",
            "      3       10.3130  0.0167\n",
            "      4       10.3130  0.0167\n",
            "      5       10.3130  0.0176\n",
            "      6       10.3130  0.0151\n",
            "      7       10.3130  0.0164\n",
            "      8       10.3130  0.0177\n",
            "      9       10.3130  0.0165\n",
            "     10       10.3130  0.0230\n",
            "     11       10.3130  0.0182\n",
            "     12       10.3130  0.0154\n",
            "     13       10.3130  0.0161\n",
            "     14       10.3130  0.0174\n",
            "     15       10.3130  0.0151\n",
            "     16       10.3130  0.0177\n",
            "     17       10.3130  0.0167\n",
            "     18       10.3130  0.0173\n",
            "     19       10.3130  0.0161\n",
            "     20       10.3130  0.0163\n",
            "     21       10.3130  0.0149\n",
            "     22       10.3130  0.0173\n",
            "     23       10.3130  0.0176\n",
            "     24       10.3130  0.0170\n",
            "     25       10.3130  0.0182\n",
            "     26       10.3130  0.0191\n",
            "     27       10.3130  0.0174\n",
            "     28       10.3130  0.0164\n",
            "     29       10.3130  0.0174\n",
            "     30       10.3130  0.0171\n",
            "     31       10.3130  0.0177\n",
            "     32       10.3130  0.0167\n",
            "     33       10.3130  0.0179\n",
            "     34       10.3130  0.0164\n",
            "     35       10.3130  0.0178\n",
            "     36       10.3130  0.0164\n",
            "     37       10.3130  0.0184\n",
            "     38       10.3130  0.0181\n",
            "     39       10.3130  0.0176\n",
            "     40       10.3130  0.0169\n",
            "     41       10.3130  0.0170\n",
            "     42       10.3130  0.0166\n",
            "     43       10.3130  0.0178\n",
            "     44       10.3130  0.0166\n",
            "     45       10.3130  0.0181\n",
            "     46       10.3130  0.0212\n",
            "     47       10.3130  0.0178\n",
            "     48       10.3130  0.0172\n",
            "     49       10.3130  0.0177\n",
            "     50       10.3130  0.0180\n",
            "     51       10.3130  0.0169\n",
            "     52       10.3130  0.0164\n",
            "     53       10.3130  0.0166\n",
            "     54       10.3130  0.0168\n",
            "     55       10.3130  0.0164\n",
            "     56       10.3130  0.0164\n",
            "     57       10.3130  0.0168\n",
            "     58       10.3130  0.0176\n",
            "     59       10.3130  0.0165\n",
            "     60       10.3130  0.0268\n",
            "     61       10.3130  0.0163\n",
            "     62       10.3130  0.0178\n",
            "     63       10.3130  0.0162\n",
            "     64       10.3130  0.0169\n",
            "     65       10.3130  0.0158\n",
            "     66       10.3130  0.0164\n",
            "     67       10.3130  0.0159\n",
            "     68       10.3130  0.0170\n",
            "     69       10.3130  0.0160\n",
            "     70       10.3130  0.0175\n",
            "     71       10.3130  0.0160\n",
            "     72       10.3130  0.0174\n",
            "     73       10.3130  0.0160\n",
            "     74       10.3130  0.0153\n",
            "     75       10.3130  0.0157\n",
            "     76       10.3130  0.0166\n",
            "     77       10.3130  0.0160\n",
            "     78       10.3130  0.0161\n",
            "     79       10.3130  0.0163\n",
            "     80       10.3130  0.0157\n",
            "     81       10.3130  0.0176\n",
            "     82       10.3130  0.0171\n",
            "     83       10.3130  0.0175\n",
            "     84       10.3130  0.0168\n",
            "     85       10.3130  0.0182\n",
            "     86       10.3130  0.0160\n",
            "     87       10.3130  0.0180\n",
            "     88       10.3130  0.0158\n",
            "     89       10.3130  0.0173\n",
            "     90       10.3130  0.0167\n",
            "     91       10.3130  0.0159\n",
            "     92       10.3130  0.0172\n",
            "     93       10.3130  0.0165\n",
            "     94       10.3130  0.0176\n",
            "     95       10.3130  0.0165\n",
            "     96       10.3130  0.0166\n",
            "     97       10.3130  0.0170\n",
            "     98       10.3130  0.0174\n",
            "     99       10.3130  0.0160\n",
            "    100       10.3130  0.0170\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m10.2768\u001b[0m  0.0151\n",
            "      2       10.2768  0.0167\n",
            "      3       10.2768  0.0168\n",
            "      4       10.2768  0.0166\n",
            "      5       10.2768  0.0160\n",
            "      6       10.2768  0.0211\n",
            "      7       10.2768  0.0183\n",
            "      8       10.2768  0.0166\n",
            "      9       10.2768  0.0188\n",
            "     10       10.2768  0.0161\n",
            "     11       10.2768  0.0182\n",
            "     12       10.2768  0.0168\n",
            "     13       10.2768  0.0184\n",
            "     14       10.2768  0.0156\n",
            "     15       10.2768  0.0168\n",
            "     16       10.2768  0.0164\n",
            "     17       10.2768  0.0174\n",
            "     18       10.2768  0.0202\n",
            "     19       10.2768  0.0205\n",
            "     20       10.2768  0.0173\n",
            "     21       10.2768  0.0174\n",
            "     22       10.2768  0.0166\n",
            "     23       10.2768  0.0173\n",
            "     24       10.2768  0.0159\n",
            "     25       10.2768  0.0167\n",
            "     26       10.2768  0.0155\n",
            "     27       10.2768  0.0170\n",
            "     28       10.2768  0.0156\n",
            "     29       10.2768  0.0149\n",
            "     30       10.2768  0.0168\n",
            "     31       10.2768  0.0164\n",
            "     32       10.2768  0.0180\n",
            "     33       10.2768  0.0176\n",
            "     34       10.2768  0.0174\n",
            "     35       10.2768  0.0178\n",
            "     36       10.2768  0.0180\n",
            "     37       10.2768  0.0172\n",
            "     38       10.2768  0.0177\n",
            "     39       10.2768  0.0165\n",
            "     40       10.2768  0.0179\n",
            "     41       10.2768  0.0172\n",
            "     42       10.2768  0.0187\n",
            "     43       10.2768  0.0171\n",
            "     44       10.2768  0.0180\n",
            "     45       10.2768  0.0215\n",
            "     46       10.2768  0.0162\n",
            "     47       10.2768  0.0160\n",
            "     48       10.2768  0.0163\n",
            "     49       10.2768  0.0183\n",
            "     50       10.2768  0.0183\n",
            "     51       10.2768  0.0196\n",
            "     52       10.2768  0.0165\n",
            "     53       10.2768  0.0174\n",
            "     54       10.2768  0.0162\n",
            "     55       10.2768  0.0175\n",
            "     56       10.2768  0.0186\n",
            "     57       10.2768  0.0186\n",
            "     58       10.2768  0.0181\n",
            "     59       10.2768  0.0179\n",
            "     60       10.2768  0.0165\n",
            "     61       10.2768  0.0175\n",
            "     62       10.2768  0.0167\n",
            "     63       10.2768  0.0174\n",
            "     64       10.2768  0.0165\n",
            "     65       10.2768  0.0166\n",
            "     66       10.2768  0.0193\n",
            "     67       10.2768  0.0168\n",
            "     68       10.2768  0.0187\n",
            "     69       10.2768  0.0174\n",
            "     70       10.2768  0.0153\n",
            "     71       10.2768  0.0167\n",
            "     72       10.2768  0.0171\n",
            "     73       10.2768  0.0176\n",
            "     74       10.2768  0.0165\n",
            "     75       10.2768  0.0194\n",
            "     76       10.2768  0.0198\n",
            "     77       10.2768  0.0180\n",
            "     78       10.2768  0.0175\n",
            "     79       10.2768  0.0153\n",
            "     80       10.2768  0.0170\n",
            "     81       10.2768  0.0171\n",
            "     82       10.2768  0.0182\n",
            "     83       10.2768  0.0173\n",
            "     84       10.2768  0.0188\n",
            "     85       10.2768  0.0170\n",
            "     86       10.2768  0.0180\n",
            "     87       10.2768  0.0174\n",
            "     88       10.2768  0.0192\n",
            "     89       10.2768  0.0205\n",
            "     90       10.2768  0.0164\n",
            "     91       10.2768  0.0176\n",
            "     92       10.2768  0.0173\n",
            "     93       10.2768  0.0190\n",
            "     94       10.2768  0.0169\n",
            "     95       10.2768  0.0182\n",
            "     96       10.2768  0.0169\n",
            "     97       10.2768  0.0177\n",
            "     98       10.2768  0.0185\n",
            "     99       10.2768  0.0170\n",
            "    100       10.2768  0.0176\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m10.2949\u001b[0m  0.0744\n",
            "      2       10.2949  0.0735\n",
            "      3       10.2949  0.0738\n",
            "      4       10.2949  0.0734\n",
            "      5       10.2949  0.0736\n",
            "      6       10.2949  0.0729\n",
            "      7       10.2949  0.0792\n",
            "      8       10.2949  0.0760\n",
            "      9       10.2949  0.0738\n",
            "     10       10.2949  0.0748\n",
            "     11       10.2949  0.0745\n",
            "     12       10.2949  0.0828\n",
            "     13       10.2949  0.0867\n",
            "     14       10.2949  0.0732\n",
            "     15       10.2949  0.0742\n",
            "     16       10.2949  0.0757\n",
            "     17       10.2949  0.0776\n",
            "     18       10.2949  0.0749\n",
            "     19       10.2949  0.0754\n",
            "     20       10.2949  0.0878\n",
            "     21        \u001b[36m4.7837\u001b[0m  0.0950\n",
            "     22        \u001b[36m0.6400\u001b[0m  0.0744\n",
            "     23        \u001b[36m0.4766\u001b[0m  0.0770\n",
            "     24        0.4959  0.0788\n",
            "     25        0.4880  0.0785\n",
            "     26        \u001b[36m0.4609\u001b[0m  0.0743\n",
            "     27        0.4764  0.0754\n",
            "     28        \u001b[36m0.4213\u001b[0m  0.0761\n",
            "     29        0.4293  0.0745\n",
            "     30        0.4276  0.0729\n",
            "     31        \u001b[36m0.4029\u001b[0m  0.0754\n",
            "     32        \u001b[36m0.3830\u001b[0m  0.0779\n",
            "     33        \u001b[36m0.3691\u001b[0m  0.0762\n",
            "     34        \u001b[36m0.3626\u001b[0m  0.0741\n",
            "     35        \u001b[36m0.3414\u001b[0m  0.0736\n",
            "     36        \u001b[36m0.3295\u001b[0m  0.0754\n",
            "     37        \u001b[36m0.3216\u001b[0m  0.0808\n",
            "     38        \u001b[36m0.3110\u001b[0m  0.0927\n",
            "     39        \u001b[36m0.3030\u001b[0m  0.0826\n",
            "     40        \u001b[36m0.2961\u001b[0m  0.0732\n",
            "     41        \u001b[36m0.2919\u001b[0m  0.0741\n",
            "     42        \u001b[36m0.2807\u001b[0m  0.0753\n",
            "     43        0.2807  0.0743\n",
            "     44        \u001b[36m0.2741\u001b[0m  0.0792\n",
            "     45        \u001b[36m0.2710\u001b[0m  0.0743\n",
            "     46        0.2716  0.0755\n",
            "     47        \u001b[36m0.2677\u001b[0m  0.0790\n",
            "     48        \u001b[36m0.2649\u001b[0m  0.0977\n",
            "     49        \u001b[36m0.2523\u001b[0m  0.0988\n",
            "     50        \u001b[36m0.2391\u001b[0m  0.0996\n",
            "     51        0.2534  0.1010\n",
            "     52        0.2562  0.1022\n",
            "     53        0.2558  0.0752\n",
            "     54        0.2431  0.0798\n",
            "     55        0.2488  0.0733\n",
            "     56        0.2495  0.0736\n",
            "     57        0.2421  0.0734\n",
            "     58        \u001b[36m0.2290\u001b[0m  0.0782\n",
            "     59        \u001b[36m0.2205\u001b[0m  0.0733\n",
            "     60        \u001b[36m0.2159\u001b[0m  0.0734\n",
            "     61        \u001b[36m0.2150\u001b[0m  0.0734\n",
            "     62        \u001b[36m0.2057\u001b[0m  0.0786\n",
            "     63        \u001b[36m0.2012\u001b[0m  0.0747\n",
            "     64        \u001b[36m0.1967\u001b[0m  0.0825\n",
            "     65        \u001b[36m0.1952\u001b[0m  0.0797\n",
            "     66        0.2008  0.0733\n",
            "     67        \u001b[36m0.1948\u001b[0m  0.0738\n",
            "     68        \u001b[36m0.1856\u001b[0m  0.0739\n",
            "     69        0.1883  0.0787\n",
            "     70        \u001b[36m0.1853\u001b[0m  0.0740\n",
            "     71        0.1881  0.0738\n",
            "     72        \u001b[36m0.1806\u001b[0m  0.0781\n",
            "     73        0.1839  0.0743\n",
            "     74        \u001b[36m0.1709\u001b[0m  0.0755\n",
            "     75        0.1775  0.0788\n",
            "     76        \u001b[36m0.1701\u001b[0m  0.0753\n",
            "     77        \u001b[36m0.1638\u001b[0m  0.0745\n",
            "     78        \u001b[36m0.1621\u001b[0m  0.0741\n",
            "     79        \u001b[36m0.1561\u001b[0m  0.0742\n",
            "     80        0.1611  0.0853\n",
            "     81        0.1583  0.0751\n",
            "     82        \u001b[36m0.1496\u001b[0m  0.0769\n",
            "     83        0.1505  0.0757\n",
            "     84        0.1508  0.0750\n",
            "     85        \u001b[36m0.1439\u001b[0m  0.0749\n",
            "     86        \u001b[36m0.1410\u001b[0m  0.0875\n",
            "     87        \u001b[36m0.1332\u001b[0m  0.0757\n",
            "     88        0.1429  0.0745\n",
            "     89        0.1337  0.0753\n",
            "     90        0.1589  0.0797\n",
            "     91        0.1452  0.0746\n",
            "     92        0.1418  0.0745\n",
            "     93        \u001b[36m0.1326\u001b[0m  0.0749\n",
            "     94        \u001b[36m0.1304\u001b[0m  0.0763\n",
            "     95        \u001b[36m0.1288\u001b[0m  0.0753\n",
            "     96        \u001b[36m0.1221\u001b[0m  0.0757\n",
            "     97        0.1356  0.0745\n",
            "     98        0.1322  0.0740\n",
            "     99        0.1315  0.0743\n",
            "    100        0.1302  0.0782\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Raq2O57MLpsH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mp = grid_search.best_params_\n",
        "mpr = grid_search.best_score_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B02XPddlL9Gm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "0d3e002b-ad3b-4f51-fa49-7db050e91706"
      },
      "source": [
        "mp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 10,\n",
              " 'criterion': torch.nn.modules.loss.BCELoss,\n",
              " 'max_epochs': 100,\n",
              " 'module__activation': <function torch.nn.functional.relu>,\n",
              " 'module__initializer': <function torch.nn.init.uniform_>,\n",
              " 'module__neurons': 16,\n",
              " 'optimizer': torch.optim.adam.Adam}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LedOuA2ZL9m9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "35775bca-2589-4cd3-a6f0-578658cc5f6c"
      },
      "source": [
        "mpr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7436125525080306"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zouRQKVtL_ZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}